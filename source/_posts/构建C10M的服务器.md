---
title: 构建C10M的服务器
date: 2017-10-14 11:45:45
time: 1507866352
tags: 
	- linux
	- mem
	- io
	- cpu
	- net
categories: Linux性能调优
comments: true
---

著名的 [C10K 问题](http://www.kegel.com/c10k.html)提出的时候, 是2001年。现在该是考虑C10M, 也就是千万连接的问题的时候了。

## 10M的并发连接挑战意味着什么： 

1. 1千万的并发连接数 
2. 100万个连接/秒；每个连接以这个速率持续约10秒 
3. 10GB/秒的连接；快速连接到互联网。 
4. 1千万个数据包/秒；据估计目前的服务器每秒处理50K的数据包，以后会更多。过去服务器每秒可以处理100K的中断，并且每一个数据包都产生中断。 
5. 10微秒的延迟；可扩展服务器也许可以处理这个规模，但延迟可能会飙升。 
6. 10微秒的抖动；限制最大延迟 
7. 并发10核技术；软件应支持更多核的服务器。通常情况下，软件能轻松扩展到四核。服务器可以扩展到更多核，因此需要重写软件，以支持更多核的服务器。

## RSS、RPS、RFS、XPS

这些技术都是近些年来为了优化Linux网络方面的性能而添加的特性，RPS、RFS、XPS都是Google贡献给社区，RSS需要硬件的支持，目前主流的网卡都已支持，即俗称的多队列网卡，充分利用多个CPU核心，让数据处理的压力分布到多个CPU核心上去。RPS和RFS在linux2.6.35的版本被加入，一般是成对使用的，在不支持RSS特性的网卡上，用软件来模拟类似的功能，并且将相同的数据流绑定到指定的核心上，尽可能提升网络方面处理的性能。XPS特性在linux2.6.38的版本中被加入，主要针对多队列网卡在发送数据时的优化，当你发送数据包时，可以根据CPU MAP来选择对应的网卡队列，低于指定的kernel版本可能无法使用相关的特性，但是发行版已经backport这些特性。

## IRQ 优化

关于IRQ的优化，这里主要有两点，第一点是关于中断合并。在比较早期的时候，网卡每收到一个数据包就会触发一个中断，如果小包的数据量特别大的时候，中断被触发的数量也变的十分可怕。大部分的计算资源都被用于处理中断，导致性能下降。后来引入了NAPI和Newernewer NAPI特性，在系统较为繁忙的时候，一次中断触发后，接下来用轮循的方式读取后续的数据包，以降低中断产生的数量，进而也提升了处理的效率。第二点是IRQ亲和性，和我们前面提到了CPU亲和性较为类似，是将不同的网卡队列中断处理绑定到指定的CPU核心上去，适用于拥有RSS特性的网卡。

这里再说说关于网络卸载的优化，目前主要有TSO、GSO、LRO、GRO这几个特性，先说说TSO，以太网MTU一般为1500，减掉TCP/IP的包头，TCP的MaxSegment Size为1460，通常情况下协议栈会对超过1460的TCP Payload进行分段，保证最后生成的IP包不超过MTU的大小，对于支持TSO/GSO的网卡来说，协议栈就不再需要这样了，可以将更大的TCPPayload发送给网卡驱动，然后由网卡进行封包操作。通过这个手段，将需要在CPU上的计算offload到网卡上，进一步提升整体的性能。GSO为TSO的升级版，不在局限于TCP协议。LRO和TSO的工作路径正好相反，在频繁收到小包时，每次一个小包都要向协议栈传递，对多个TCPPayload包进行合并，然后再传递给协议栈，以此来提升协议栈处理的效率。GRO为LRO的升级版本，解决了LRO存在的一些问题。这些特性都是在一定的场景下才可以发挥其性能效率，在不明确自己的需求的时候，开启这些特性反而可能造成性能下降。

## 操作系统最大打开文件数限制

> 分为系统全局的, 和进程级的限制.

### 全局限制

```
# cat /proc/sys/fs/file-nr
960	0	98400
```

第三个数字 98400 就是当前系统的全局最大打开文件数(Max Open Files), 可以看到只有1万, 所以在这台服务器上无法支持 C1000K。

为了修改这个数值, 用 root 权限修改 /etc/sysctl.conf 文件:

```
fs.file-max = 1020000
net.ipv4.ip_conntrack_max = 1020000
net.ipv4.netfilter.ip_conntrack_max = 1020000
```

重启系统服务生效:

```
$ sudo sysctl -p /etc/sysctl.conf
```

### 进程限制

```
# ulimit -n
65535
```

说明当前 Linux 系统的每一个进程只能最多打开 65535 个文件. 为了支持 C1000K, 你同样需要修改这个限制。

> 临时修改

```
#ulimit -n 1020000
```

> 永久修改

编辑 /etc/security/limits.conf 文件, 加入如下行:

```
work         hard    nofile      1020000
work         soft    nofile      1020000
```

第一列的 work 表示 work 用户，可以填 * 或者 root。然后保存退出，重新登录服务器.

> PS: Linux 内核源码中有一个常量(NR_OPEN in /usr/include/linux/fs.h), 限制了最大打开文件数, 如 RHEL 5 是 1048576(2^20), 所以, 要想支持 [C1000K](http://www.ideawu.net/blog/tag/c1000k), 你可能还需要重新编译内核.

## 系统维持千万连接需要多少内存?

首先, 是操作系统本身维护这些连接的内存占用. 对于 Linux 操作系统, socket(fd) 是一个整数, 所以, 猜想操作系统管理一百万个连接所占用的内存应该是 4M/8M, 再包括一些管理信息, 应该会是 100M 左右. 不过, 还有 socket 发送和接收缓冲区所占用的内存没有分析. 为此, 我写了最原始的 C 网络程序来验证:

### 服务器

```
\#include <stdio.h>
\#include <stdlib.h>
\#include <string.h>
\#include <unistd.h>
\#include <errno.h>
\#include <arpa/inet.h>
\#include <netinet/tcp.h>
\#include <sys/select.h>

#define MAX_PORTS 10

int main(int argc, char **argv){
​    struct sockaddr_in addr;
​    const char *ip = "0.0.0.0";
​    int opt = 1;
​    int bufsize;
​    socklen_t optlen;
​    int connections = 0;
​    int base_port = 7000;

​    if(argc > 2){
​        base_port = atoi(argv[1]);
​    }

​    int server_socks[MAX_PORTS];

​    for(int i=0; i<MAX_PORTS; i++){
​        int port = base_port + i;
​        bzero(&addr, sizeof(addr));
​        addr.sin_family = AF_INET;
​        addr.sin_port = htons((short)port);
​        inet_pton(AF_INET, ip, &addr.sin_addr);

​        int serv_sock;

​        if((serv_sock = socket(AF_INET, SOCK_STREAM, 0)) == -1){
​            goto sock_err;
​        }

​        if(setsockopt(serv_sock, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt)) == -1){
​            goto sock_err;
​        }

​        if(bind(serv_sock, (struct sockaddr *)&addr, sizeof(addr)) == -1){
​            goto sock_err;
​        }

​        if(listen(serv_sock, 1024) == -1){
​            goto sock_err;
​        }

​        server_socks[i] = serv_sock;
​        printf("server listen on port: %d\n", port);
​    }

​    while(1){
​        fd_set readset;
​        FD_ZERO(&readset);
​        int maxfd = 0;

​        for(int i=0; i<MAX_PORTS; i++){
​            FD_SET(server_socks[i], &readset);
​            if(server_socks[i] > maxfd){
​                maxfd = server_socks[i];
​            }
​        }

​        int ret = select(maxfd + 1, &readset, NULL, NULL, NULL);

​        if(ret < 0){
​            if(errno == EINTR){
​                continue;
​            }else{
​                printf("select error! %s\n", strerror(errno));
​                exit(0);
​            }
​        }

​        if(ret > 0){
​            for(int i=0; i<MAX_PORTS; i++){
​                if(!FD_ISSET(server_socks[i], &readset)){
​                    continue;
​                }

​                socklen_t addrlen = sizeof(addr);

​                int sock = accept(server_socks[i], (struct sockaddr *)&addr, &addrlen);

​                if(sock == -1){
​                    goto sock_err;
​                }

​                connections ++;
​                printf("connections: %d, fd: %d\n", connections, sock);
​            }
​        }
​    }

​    return 0;

sock_err:

​    printf("error: %s\n", strerror(errno));
​    return 0;
}
```

注意, 服务器监听了 10 个端口, 这是为了测试方便. 因为只有一台客户端测试机, 最多只能跟同一个 IP 端口创建 30000 多个连接, 所以服务器监听了 10 个端口, 这样一台测试机就可以和服务器之间创建 30 万个连接了.

### 客户端

```
#include <stdio.h>
\#include <stdlib.h>
\#include <string.h>
\#include <unistd.h>
\#include <errno.h>
\#include <arpa/inet.h>
\#include <netinet/tcp.h>

int main(int argc, char **argv){

    if(argc <=  2){
        printf("Usage: %s ip port\n", argv[0]);
        exit(0);

    }

    struct sockaddr_in addr;
    const char *ip = argv[1];
    int base_port = atoi(argv[2]);
    int opt = 1;
    int bufsize;
    socklen_t optlen;
    int connections = 0;
    bzero(&addr, sizeof(addr));
    addr.sin_family = AF_INET;
    inet_pton(AF_INET, ip, &addr.sin_addr);

    char tmp_data[10];
    int index = 0;

    while(1){
        if(++index >= 10){
            index = 0;
        }

        int port = base_port + index;
        printf("connect to %s:%d\n", ip, port);
        addr.sin_port = htons((short)port);
        int sock;

        if((sock = socket(AF_INET, SOCK_STREAM, 0)) == -1){
            goto sock_err;
        }

        if(connect(sock, (struct sockaddr *)&addr, sizeof(addr)) == -1){
            goto sock_err;
        }

        connections ++;
        printf("connections: %d, fd: %d\n", connections, sock);

        if(connections % 10000 == 9999){
            printf("press Enter to continue: ");
            getchar();
        }

        usleep(1 * 1000);

        /*
           bufsize = 5000;
           setsockopt(serv_sock, SOL_SOCKET, SO_SNDBUF, &bufsize, sizeof(bufsize));
           setsockopt(serv_sock, SOL_SOCKET, SO_RCVBUF, &bufsize, sizeof(bufsize));
         */

    }

    return 0;

sock_err:
    printf("error: %s\n", strerror(errno));
    return 0;
}
```

我测试 10 万个连接, 这些连接是空闲的, 什么数据也不发送也不接收. 这时, 进程只占用了不到 1MB 的内存. 但是, 通过程序退出前后的 free 命令对比, 发现操作系统用了 200M(大致)内存来维护这 10 万个连接! 如果是百万连接的话, 操作系统本身就要占用 2GB 的内存! 也即 2KB 每连接.

可以修改:
```
/proc/sys/net/ipv4/tcp_wmem
/proc/sys/net/ipv4/tcp_rmem
```
来控制 TCP 连接的发送和接收缓冲的大小。

## 应用程序维持千万连接需要多少内存?

通过上面的测试代码, 可以发现, 应用程序维持百万个空闲的连接, 只会占用操作系统的内存, 通过 ps 命令查看可知, 应用程序本身几乎不占用内存.

## 千万连接的吞吐量是否超过了网络限制?

假设百万连接中有 20% 是活跃的, 每个连接每秒传输 1KB 的数据, 那么需要的网络带宽是 0.2M x 1KB/s x 8 = 1.6Gbps, 要求服务器至少是万兆网卡(10Gbps).


## 实现数据包可扩展——编写自己的个性化驱动来绕过堆栈 

数据包的问题是它们需经Unix内核的处理。网络堆栈复杂缓慢，数据包最好直接到达应用程序，而非经过操作系统处理之后。
做到这一点的方法是编写自己的驱动程序。所有驱动程序将数据包直接发送到应用程序，而不是通过堆栈。你可以找到这种驱动程序：PF_RING，NETMAP，Intel DPDK（数据层开发套件）。Intel不是开源的，但有很多相关的技术支持。
速度有多快？Intel的基准是在一个相当轻量级的服务器上，每秒处理8000万个数据包（每个数据包200个时钟周期）。这也是通过用户模式。将数据包向上传递，使用用户模式，处理完毕后再返回。Linux每秒处理的数据包个数不超过百万个，将UDP数据包提高到用户模式，再次出去。客户驱动程序和Linux的性能比是80：1。
对于每秒1000万个数据包的目标，如果200个时钟周期被用来获取数据包，将留下1400个时钟周期实现类似DNS / IDS的功能。
通过PF_RING得到的是原始数据包，所以你必须做你的TCP堆栈。人们所做的是用户模式栈。Intel有现成的可扩展TCP堆栈

## 多核的可扩展性 

多核可扩展性不同于多线程可扩展性。我们都熟知这个理念：处理器的速度并没有变快，我们只是靠增加数量来达到目的。 
大多数的代码都未实现4核以上的并行。当我们添加更多内核时，下降的不仅仅是性能等级，处理速度可能也会变得越来越慢，这是软件的问题。我们希望软件的提高速度同内核的增加接近线性正相关。 

多线程编程不同于多核编程
- 多线程
  - 每个CPU内核中不止一个线程
  - 用锁来协调线程（通过系统调用）
  - 每个线程有不同的任务
- 多核
  - 每个CPU内核中只有一个线程
  - 当两个线程/内核访问同一个数据时，不能停下来互相等待

同一个任务的不同线程:

- 要解决的问题是怎样将一个应用程序分布到多个内核中去
- Unix中的锁在内核实现。4内核使用锁的情况是大多数软件开始等待其他线程解锁。因此，增加内核所获得的收益远远低于等待中的性能损耗。
- 我们需要这样一个架构，它更像高速公路而不是红绿灯控制的十字路口，无需等待，每个人都以自己的节奏行进，尽可能节省开销。

解决方案：

- 在每个核心中保存数据结构，然后聚合的对数据进行读取。
- 原子性。CPU支持可以通过C语言调用的指令，保证原子性，避免冲突发生。开销很大，所以不要处处使用。
- 无锁的数据结构。线程无需等待即可访问，在不同的架构下都是复杂的工作，请不要自己做。
- 线程模型，即流水线与工作线程模型。这不只是同步的问题，而是你的线程如何架构。
- 处理器关联。告诉操作系统优先使用前两个内核，然后设置线程运行在哪一个内核上，你也可以通过中断到达这个目的。所以，CPU由你来控制而不是Linux。

## 内存的可扩展性 

如果你有20G的RAM，假设每次连接占用2K的内存，如果你还有20M的三级缓存，缓存中会没有数据。数据转移到主存中处理花费300个时钟周期，此时CPU没有做任何事情。
每个数据包要有1400个时钟周期（DNS / IDS的功能）和200个时钟周期（获取数据包）的开销，每个数据包我们只有4个高速缓存缺失，这是一个问题。

联合定位数据
不要通过指针在满内存乱放数据。每次你跟踪一个指针，都会是一个高速缓存缺失：[hash pointer] -> [Task Control Block] -> [Socket] -> [App]，这是四个高速缓存缺失。
保持所有的数据在一个内存块：[TCB |socket| APP]。给所有块预分配内存，将高速缓存缺失从4减少到1。
分页
32GB的数据需占用64MB的分页表，不适合都存储在高速缓存。所以存在两个高速缓存缺失——分页表和它所指向的数据。这是开发可扩展的软件不能忽略的细节。
解决方案：压缩数据，使用有很多内存访问的高速缓存架构，而不是二叉搜索树
NUMA架构加倍了主存访问时间。内存可能不在本地socket，而是另一个socket上。
内存池
启动时立即预先分配所有的内存
在对象，线程和socket的基础上进行分配。
超线程
每个网络处理器最多可以运行4个线程，英特尔只能运行2个。
在适当的情况下，我们还需要掩盖延时，比如内存访问中一个线程在等待另一个全速的线程。
大内存页
减小页表规模。从一开始就预留内存，让你的应用程序管理内存。
总结

## 网卡
问题：

​	通过内核工作效率不高

解决方案：

​	使用自己的驱动程序并管理它们，使适配器远离操作系统。


## CPU
问题：

​	使用传统的内核方法来协调你的应用程序是行不通的。
解决方案：

​	Linux管理前两个CPU，你的应用程序管理其余的CPU。中断只发生在你允许的CPU上。

## 内存

问题：

​	内存需要特别关注，以求高效。
解决方案：

​	在系统启动时就分配大部分内存给你管理的大内存页

控制层交给Linux，应用程序管理数据。应用程序与内核之间没有交互，没有线程调度，没有系统调用，没有中断，什么都没有。 
然而，你有的是在Linux上运行的代码，你可以正常调试，这不是某种怪异的硬件系统，需要特定的工程师。你需要定制的硬件在数据层提升性能，但是必须是在你熟悉的编程和开发环境上进行。 



![img](http://upload-images.jianshu.io/upload_images/1627454-bd6206c5fa44dd16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)