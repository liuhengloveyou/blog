---
title: 网站反爬虫策略
comments: true
date: 2017-11-21 12:37:03
time: 1511239023
tags: 爬虫
categories: web
---


## 目的

给不受欢迎的爬虫制造麻烦。最好让它们知难而退，放弃我的网站数据，去爬别人。

不论是哪种攻防战，都是一种成本与收益的权衡。双方比拼谁的道行更高，进化更快速。道行高的一方能够快速方便的做出一些低成本的改变，让对方花高出很多的成本去应对，就能压制对方。

反爬虫的本质是准确的区分人类用户和爬虫程序的行为区别，限制爬虫程序的访问。理论上只要爬虫程序可以模拟正常人类上网行为，就可以击败反爬虫策略。

所有在这场攻防战中，爬虫一方是有先天优势的。




## 策略

### 限IP / 限用户/ 限cookie/ 限UA / 限Refer

…各种字段限， 本质是一样的， 就是对请求的某个字段做限制。

比如ip黑名单限制某些ip不能访问。限某些用户有权限或没权限。限只有某些UA/Refer能访问之类。

这是最简单的办法， 随手改一个字段就可以实现。当然也最容易攻破，我想应该基本没什么效果了，哪个写爬虫的不会分析一下协议呢。试出规则改一下字段值就行了。**攻破成本太低了**。



### 限访问频率

这应该是现在用得比较多的策略。最常用的是限单个IP、单个用户/会话的访问频率。比如每分钟请求大于100次屏蔽IP，每小时请求大于1000屏蔽IP。

这种策略已经触及反爬虫的本质了，到底什么样的请求特征是人类？什么样的请求特征是爬虫程序呢？如果阈值严了，会误伤真实人类用户。如果阈值松了，又起不到好的防爬效果。难点就在这里。

不过，我们可以不用做得那么高大上， 去大数据分析人类的请求行为模式。用一些简单的假设区分真实人类行为，就可以达到很好的防爬虫效果。比如：

- 人不会长时间持续访问一个网站。
- 人访问一个网站，必定会请求页面上的所有资源， 而不是只有某一种数据。

所以，我们可以简化策略，设置多个限制周期。短周期宽松，长周期严格。

这样爬虫运行越久， 就会碰到越严格的限制。成本会慢慢增大，速度越快就被限的越快。如果放慢速度，那数据更新就慢。



### 验证码

验证码的初衷就是区分人类和程序。可以在入口处，和怀疑是恶意爬虫请求的时候， 要求输入验证吗。

在防爬虫这个战争中， 所面临的挑战也是一样的。什么样的请求是真实人类用户的请求呢？验证码太复杂，跳得太频繁，肯定影响用户体验。否则效果不好， 更何况现在验证分析那么强大。



###  数据加密

这是另一种处理模式，在一定程度上绕过了 **是不是真实人类用户请求** 这个问题。

做法是加密应该给客户端的数据，例如我很久前写的一个[nginx模块](https://github.com/liuhengloveyou/ngx_http_rc4_filter_module)，客户端解密以后再渲染。或者客户端动态生成token，错误的token不给应答。

这个方案拼的是数据加密的技术了，客户端的加密库要做混淆。算法被攻破以后要升级版本。